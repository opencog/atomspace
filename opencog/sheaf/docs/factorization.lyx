#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Factoring with Statistical Linguistics
\end_layout

\begin_layout Date
January 2023
\end_layout

\begin_layout Author
Linas Vep≈°tas
\end_layout

\begin_layout Abstract
An attempt is made to develop a mathematical formalism for factoring large
 language graphs into factors that have a symbolic interpretation.
 The OpenCog language learning effort has been attempting to induce grammar,
 syntax and semantics from corpora.
 Most of this work is purely experimental, 
\begin_inset Quotes eld
\end_inset

seat of the pants
\begin_inset Quotes erd
\end_inset

 exploration.
 This document attempts to provide a mathematical notation for that work,
 thus perhaps making it clearer and easier to grasp.
\end_layout

\begin_layout Abstract
The primary focus is on exploring the nature of probability in extremely
 high-dimensional spaces (
\begin_inset Quotes eld
\end_inset

hyperspaces
\begin_inset Quotes erd
\end_inset

), and how traditional linguistic ideas can be applied to factorize probability
 distributions into components.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
A probabilistic description of natural language posits that probability
 theory can be validly applied to word-sequences.
 Given a sequence of words 
\begin_inset Formula $\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 representing a sentence, a paragraph, or a longer text..., one make the
\emph on
 a priori
\emph default
 assummption that it is possible to assign a probability 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 to this sequence.
 It is not philosophically or scientifically obvious that this is a valid
 assumption: the collection of sayable sentences is presumably infinite;
 language changes over time; ever human speaker internalizes slightly different
 grammars and idiomatic expressions.
 Vocabularies are different for technical texts and literary texts.
 While it is true that present-day computers can gather up billions of sentences
 by scraping the web, it can hardly be assumed that these are converging
 to some overall stable probability distribution 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

.
 Thus, assuming that 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 exists is intellectually dangerous.
\end_layout

\begin_layout Standard
None-the-less, we make this assumption, for two reasons.
 First, it is useful.
 Second, if a specific finite-sized corpus is selected and fixed, it is
 a simple and unambiguous matter of counting words and phrases to obtain
 frequency distributions.
 However, even in this case, one should not be naive: the probability space
 of of all sentences in a modest sized corpus is immense.
 It would be nice if one could work with smallar factors.
 Linguists have already exposed what these factors could be: nouns, verbs,
 grammatical relationships.
 One goal of this text is to formalize the relationship between grammar
 and proability spaces, using mathematical notation rather than hot air.
 It is hoped this will make things clearer.
 Another goal is to extend this analysis into the domain of semantics and
 
\begin_inset Quotes eld
\end_inset

common sense
\begin_inset Quotes erd
\end_inset

.
 This second goal won't be met, other than to suggest that exactly the same
 methods that allowed low-level syntactic factorization to be performed,
 can also be applied, again, at more abstract levels.
 A third goal is to use this mathematical machinery to guide the development
 of software for performing this analysis, to guide future experiments,
 and to provde a better theoretical foundation for what has so far been
 a seat-of-the-pants effort.
\end_layout

\begin_layout Subsection*
Parsing
\end_layout

\begin_layout Standard
The seat-of-the-pants effort so far has been forcused on the automated extractio
n of a grammar from a text corpus.
 Issues of text segmentation are completely avoided: it is assumed that
 the corpus consists of words, unabiguously separated by blank spaces.
 These are avoided because segmentation is a deep, difficult and interesting
 problem, and tackling it takes us afield.
 Likewise, issues of morphology are also ignored.
 Both of these are fundamentally important.
 It is hoped (beleived by the author) that the techniques described here
 will also be applicable there.
 But for now, its easiest to presume that there is a text corpus, consisting
 of well-defined 
\begin_inset Quotes eld
\end_inset

words
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
One statistical approach to parsing is to simply count word-pairs in the
 sample corpus, and then to compute the pairwise point mutual information
 
\begin_inset Formula $MI\left(u,w\right)$
\end_inset

 for all pairs.
 This mutual information can be used to create a Maximum Spanning Tree (MST)
 parse: to consider all possible trees spanning all words in a sentence
 (or block of text) and then select the one that maximizes the grand total
 MI, summed over the word-pairs in the tree.
 It is also useful to consider the Maximal Planar Graph (MPG) parse: starting
 with the MST tree, add edges to create cycles (loops), while still maximizing
 the total MI.
 That such MST parses correspond to reasonable linguistic structure has
 been widely explored over several decades.
\end_layout

\begin_layout Standard
A grammar can be extracted by taking such MST/MPG parses and cutting each
 edge in half, and retaining, as a 
\begin_inset Quotes eld
\end_inset

connector label
\begin_inset Quotes erd
\end_inset

, what word that edge used to connect to.
 The result of such chopping-up are the so-called 
\begin_inset Quotes eld
\end_inset

disjuncts
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

jigsaw puzzle pieces
\begin_inset Quotes erd
\end_inset

 of Link Grammar.
 These can be reassmbled again, to obtain syntactic parses of sentences.
 Link Grammar works: there are extensive hand-curated dictionaries for English,
 Russian and Thai, with smaller dictionaries for another dozen natural languages.
 The English dictionary might be the most accurate/sophisticated parsing
 system currently available.
 
\end_layout

\begin_layout Standard
Link Grammar grammars can be converted to other formalisms; 
\emph on
e.g.

\emph default
 Head-Phrase Sturcture Grammar (HPSG) and so on.
 It can be shown that Link Grammar is 
\begin_inset Quotes eld
\end_inset

isomorphic
\begin_inset Quotes erd
\end_inset

 to Combinatory Categorial Grammars (CCG).
 The quotes around 
\begin_inset Quotes eld
\end_inset

isomorphic
\begin_inset Quotes erd
\end_inset

 have less to do about the math, than what a typical linguist might find
 acceptable in the mapping.
 For the remainder of this text, we assume that any grammar formalism is
 acceptable, and that they are all inter-convertible, interchangeable with
 one another, at least weakly, if not strongly.
 The goal of this text is to expose the relationship between statistics
 and grammar, rather than to quibble the finer points of linguistics.
 When the text below says things like 
\begin_inset Quotes eld
\end_inset

a relationship 
\begin_inset Formula $r\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 between three words 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

 you are free to imagine any grammar formlism that you wish, involving subjects,
 verbs and objects and so on.
 However, Link Grammar will remain the touchstone, as it is the most compatible
 with probability theory.
 Thus, a general acquiantance with Link Grammar is strongly recommended.
\end_layout

\begin_layout Subsection*
Literature Review
\end_layout

\begin_layout Standard
The goal of the present text is to talk about the factorization of graphs,
 in general.
 There has been, of course, much related prior work.
\end_layout

\begin_layout Standard
The idea of statistical parsing
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Statistical parsing"
target "https://en.wikipedia.org/wiki/Statistical_parsing"
literal "false"

\end_inset

.
\end_layout

\end_inset

 has been around for decades.
 Among the earliest work is Charniak's Maximum Entropy Parser.
\begin_inset CommandInset citation
LatexCommand cite
key "Charniak2000"
literal "false"

\end_inset

 No lit review, XXX reference wikipedia instead? 
\end_layout

\begin_layout Standard
The idea of matrix factorization is central large consumer businesses, who
 wish to estimate future shopping patterns as a function of prior behavior.
 Vast numbers of papers have been written...
\end_layout

\begin_layout Standard
Hypervectors are a relatively newer approach to computing ...
\end_layout

\begin_layout Section*
Factorization
\end_layout

\begin_layout Standard
The notion of factorization is to take some large blob, and pick it apart
 into components: to factor a matrix into block-diagonal components, to
 factor an integer into primes.
 Probability distributions over statistically independent variables factorize
 trivially: this is what is meant by the words 
\begin_inset Quotes eld
\end_inset

statistically indepenent
\begin_inset Quotes erd
\end_inset

.
 Probability distributions over language are not, of course, statistically
 independent, and thus are not strictly factorizable.
 None-the-less, they are almost so; the goal is to identify the strongly
 connected components and separate them from one-another, identifying the
 weaker connections.
\end_layout

\begin_layout Standard
Lets try to capture this idea using mathematical notation.
 To recap the story so far: Let 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 be the probability of observing 
\begin_inset Formula $n$
\end_inset

 words in a sentence (or block of text).
 The space of sequences 
\begin_inset Formula $\left\{ \left(w_{1},w_{2},\cdots,w_{n}\right)\right\} $
\end_inset

 is a Cartesian product space, and 
\begin_inset Formula $p$
\end_inset

 is a measure upon that space.
 
\end_layout

\begin_layout Standard
The goal of factorization is to approximate this measure by factorizing
 it into parts, where the parts are given by parsing via conventional linguistic
 theory.
 That is, we presume that relations 
\begin_inset Formula $r_{i}$
\end_inset

 between small sets of words can be found, such that the following holds,
 approximately:
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(r_{1}\left\{ w\right\} \right)p\left(r_{2}\left\{ w\right\} \right)\cdots p\left(r_{k}\left\{ w\right\} \right)
\]

\end_inset

where 
\begin_inset Formula $r_{1},r_{2},\cdots,r_{k}$
\end_inset

 are syntactic relations (subject, verb, object...) and the 
\begin_inset Formula $\left\{ w\right\} $
\end_inset

 are the set of words taking part in that particular relationship.
 For example, the relation might be a subject-verb-object relationship;
 the set 
\begin_inset Formula $\left\{ w\right\} $
\end_inset

 then consists of only three words.
 The point here is that the 
\begin_inset Formula $r_{i}$
\end_inset

 are 
\begin_inset Quotes eld
\end_inset

small
\begin_inset Quotes erd
\end_inset

, whereas 
\begin_inset Formula $\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

large
\begin_inset Quotes erd
\end_inset

.
 The goal is to grapple wth complexity be finding sutiable recurring patterns.
 Lingusits have already shown what these patterns should be; now the task
 is to actually extract them from text.
\end_layout

\begin_layout Standard
The factorization is successful if 
\begin_inset Formula 
\[
\log_{2}\frac{p\left(w_{1},w_{2},\cdots,w_{n}\right)}{p\left(r_{1}\right)p\left(r_{2}\right)\cdots p\left(r_{k}\right)}\approx0
\]

\end_inset

With such a factorization in hand, one can now aim for higher and more abstract
 levels of structure, using the 
\begin_inset Formula $r_{i}$
\end_inset

 as the building blocks, rather than individual words.
 One should imagine a perturbative structure, each level givein a foundation
 for the next.
\end_layout

\begin_layout Subsection*
Example: the Binomial MI Formula
\end_layout

\begin_layout Standard
As a concrete example of the above, consider the mutual information 
\begin_inset Formula $MI\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 over 
\begin_inset Formula $n$
\end_inset

 variables.
 It is defined as
\begin_inset Formula 
\begin{equation}
MI\left(w_{1},w_{2},\cdots,w_{n}\right)=\sum_{k=0}^{n}\left(-1\right)^{n-k}\sum_{w\backslash k}\log_{2}p\left(\left\{ w\backslash k\right\} \right)\label{eq:binomial-MI}
\end{equation}

\end_inset

where 
\begin_inset Formula $\left\{ w\backslash k\right\} $
\end_inset

 is the set of words 
\begin_inset Formula $\left\{ w\right\} =\left\{ w_{1},w_{2},\cdots,w_{n}\right\} $
\end_inset

 with 
\begin_inset Formula $k$
\end_inset

 of them removed.
 The sum over 
\begin_inset Formula $w\backslash k$
\end_inset

 is a sum over every combinatoric possibility of removal.
 By 
\begin_inset Quotes eld
\end_inset

removed
\begin_inset Quotes erd
\end_inset

, it is meant 
\begin_inset Quotes eld
\end_inset

summed over
\begin_inset Quotes erd
\end_inset

, so that, for example, if 
\begin_inset Formula $w_{2}$
\end_inset

 is removed, then
\begin_inset Formula 
\[
p\left(\left\{ w\backslash w_{2}\right\} \right)=p\left(w_{1},*,w_{3},\cdots,w_{n}\right)=\sum_{w_{2}}p\left(w_{1},w_{2},w_{3},\cdots,w_{n}\right)
\]

\end_inset

The 
\begin_inset Formula $*$
\end_inset

 is the wild-card; it just denotes that 
\begin_inset Quotes eld
\end_inset

anything
\begin_inset Quotes erd
\end_inset

 can occupy that slot, and that, for probabilites, that slot should be summed
 over.
 Formally, 
\begin_inset Formula $\left\{ w\backslash w_{2}\right\} =\left(w_{1},*,w_{3},\cdots,w_{n}\right)$
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

cylinder set
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Formula $p\left(\left\{ w\backslash w_{2}\right\} \right)$
\end_inset

 is a cylinder set measure.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia,
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset href
LatexCommand href
name "Cylinder set"
target "https://en.wikipedia.org/wiki/Cylinder_set"
literal "false"

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset href
LatexCommand href
name "Cylinder set measure"
target "https://en.wikipedia.org/wiki/Cylinder_set_measure"
literal "false"

\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sum over 
\begin_inset Formula $w\backslash k$
\end_inset

 for 
\begin_inset Formula $k=1$
\end_inset

 is then a sum over all possible wildcard locations, for a single wildcard:
\begin_inset Formula 
\[
\sum_{w\backslash1}\log_{2}p\left(\left\{ w\backslash1\right\} \right)=\sum_{i=1}^{n}\log_{2}p\left(\left\{ w\backslash w_{i}\right\} \right)
\]

\end_inset

Likewise, for 
\begin_inset Formula $k=2$
\end_inset

 wildcards,
\begin_inset Formula 
\[
\sum_{w\backslash2}\log_{2}p\left(\left\{ w\backslash2\right\} \right)=\sum_{i=1}^{n}\sum_{j=1;j\ne i}^{n}\log_{2}p\left(\left\{ w\backslash\left\{ w_{i},w_{j}\right\} \right\} \right)
\]

\end_inset

and so on.
\end_layout

\begin_layout Standard
The alternating sign is such that the singletons 
\begin_inset Formula $p\left(w_{j}\right)=p\left(*,*,\cdots,w_{j},\cdots,*\right)$
\end_inset

 always have a minus sign in front of their log, while the first term is
 for the total space 
\begin_inset Formula $p\left(\left\{ w\backslash w\right\} \right)=p\left(\left\{ \varnothing\right\} \right)=p\left(*,*,\cdots,*\right)$
\end_inset

.
 If 
\begin_inset Formula $p\left(\left\{ \varnothing\right\} \right)=1$
\end_inset

 is a conventional probability, then of course 
\begin_inset Formula $\log_{2}p\left(\left\{ \varnothing\right\} \right)=0$
\end_inset

.
 However, this MI sum works just fine if 
\begin_inset Formula $p\left(\left\{ \varnothing\right\} \right)\ne1$
\end_inset

.
 For example, the binomial formula eqn 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:binomial-MI"
plural "false"
caps "false"
noprefix "false"

\end_inset

 still holds if 
\begin_inset Formula $N$
\end_inset

 a count is used instead of 
\begin_inset Formula $p$
\end_inset

.
 This is because the normalizing factor 
\begin_inset Formula $N\left(\left\{ \varnothing\right\} \right)$
\end_inset

 can be pulled back through all of the terms.
\end_layout

\begin_layout Standard
The size of the set 
\begin_inset Formula $\left\{ w\backslash k\right\} $
\end_inset

 is given by the binomial coefficient:
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Binomial coefficient"
target "https://en.wikipedia.org/wiki/Binomial_coefficient"
literal "false"

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
\left|\left\{ w\backslash k\right\} \right|={n \choose k}
\]

\end_inset

Note the resemblance of the formula for MI to the binomial theorem.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Binomial Theorem"
target "https://en.wikipedia.org/wiki/Binomial_theorem"
literal "false"

\end_inset


\end_layout

\end_inset

 This is not accidental; it is a generalization of the binomial formula
 that holds for non-uniform intervals and non-independent correlations.
 It reduces to exactly the binomial formula if all probabilities are independent
 and uniform in size,
\emph on
 i.e.

\emph default
 if 
\begin_inset Formula $p\left(a,b\right)=p\left(a\right)p\left(b\right)$
\end_inset

 and if 
\begin_inset Formula $p\left(w_{j}\right)=1/n$
\end_inset

.
 In this case, it becomes
\begin_inset Formula 
\begin{align*}
MI\left(w_{1},w_{2},\cdots,w_{n}\right)= & \sum_{k=0}^{n}\left(-1\right)^{n-k}{n \choose k}\log_{2}\frac{1}{n^{k}}\\
= & -\log_{2}n\cdot\sum_{k=0}^{n}\left(-1\right)^{n-k}k{n \choose k}\\
= & 0
\end{align*}

\end_inset

The last part follows from 
\begin_inset Formula 
\[
\frac{d}{dx}\left(1+x\right)^{n}=n\left(1+x\right)^{n-1}=\sum_{k=1}^{n}k{n \choose k}x^{k-1}
\]

\end_inset

and setting 
\begin_inset Formula $x=-1$
\end_inset

.
 
\end_layout

\begin_layout Standard
More generally, the binomial-MI formula follows from the Cartesian-product
 nature of the topology of sequences.
 It is a formula that holds generically for cylinder set measures; there
 is nothing language-specific in this example.
\end_layout

\begin_layout Standard
The point of this example is to show that something seemingly 
\begin_inset Quotes eld
\end_inset

large
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

complex
\begin_inset Quotes erd
\end_inset

, such as 
\begin_inset Formula $MI\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 can be reduced into smaller, perhaps more manageable components, which
 can then be recombined back into the whole, with an exact (not approximate)
 expression, a summation over pieces-parts.
\end_layout

\begin_layout Standard
But there is also a second lesson here: the binomial MI formula is not sutiable
 for natural language tasks.
 Although it is an exact expression, and individual words and word pairs
 appear in the lower summation terms, one gains no insight applying this
 to natural language.
 MI values can be both negative and positive; the alternating sign introduces
 more chaos into the mix.
 Basically, one has a series of small and large terms summing up and mostly
 cancelling one-another.
 (Literally: try this experimentally, if possible.
 You will find that the various MI's bounce around, getting large and small,
 and that the sum is almost always smaller than the largest term.) The quest
 is to find a similar expression, ideally, an exact expression, where most
 of the terms are strictly positive.
 This would allow the structure, the factorization to be approached perturbative
ly, as a strictly convergent sequence of corrections, each applied to the
 last.
\end_layout

\begin_layout Subsection*
Example: Syntactic Factorization
\end_layout

\begin_layout Standard
Parses imply factorizations.
 Consider a sentence with a fixed single parse.
 Suppose that there is a location 
\begin_inset Formula $i$
\end_inset

 in this parse, such that when considering the block of all words to the
 left of 
\begin_inset Formula $i$
\end_inset

, and the block of all words to the right of 
\begin_inset Formula $i$
\end_inset

, there is only a single edge connecting the two sides.
 For example, this might be the edge connecting word 
\begin_inset Formula $w_{i}$
\end_inset

, say, the verb, to word 
\begin_inset Formula $w_{j}$
\end_inset

, say, the object.
 (One cannot assume that 
\begin_inset Formula $j=i+1$
\end_inset

 since the object might have adjectives and determiners that precede it.
 One should assume that 
\begin_inset Formula $i<j$
\end_inset

.) Then this parse implies a factorization 
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(w_{1},\cdots,w_{i}\right)p\left(r\left\{ w_{i},w_{j}\right\} \right)p\left(w_{i+1},\cdots,w_{n}\right)
\]

\end_inset

The is, the likelihood of the block of words to the left is effectively
 independent of the block of words on the right.
 
\end_layout

\begin_layout Standard
This factorization follows from the intuitive grammatical structure of natural
 language.
 Consider the sentence fragment 
\begin_inset Quotes eld
\end_inset

On alternate Tuesdays, John goes ...
\begin_inset Quotes erd
\end_inset

 How can it be completed? One imagines almost anything can complete it:
 
\begin_inset Quotes eld
\end_inset

...
 fishing in Georgetown.
\begin_inset Quotes erd
\end_inset

 
\begin_inset Quotes eld
\end_inset

...
 to the doctor.
\begin_inset Quotes erd
\end_inset

 That is, the completion of the sentence seems indpendent of the start of
 the sentence, and so the probability expression should factor like this
 as well.
\end_layout

\begin_layout Standard
Yet, this is just an approximation.
 Realizing that the first half of the sentence implies activity undertaken
 by a human, then the last half of the sentence must be an activity that
 humans can perform.
 So there is a linkage, a connection between these two parts of the sentence
 that extend beyond the grammatical relations between pairs of words.
 So again: the proposal here is to first factor according to syntax, providing
 a baseline, and then consider corrections to that initial factorization.
\end_layout

\begin_layout Standard
The above was written with a factor 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 specifically tying together the two specific words 
\begin_inset Formula $w_{i},w_{j}$
\end_inset

 connected by the parse edge.
 This factor is made explicit because one imagines that the specific word-choice
 connecting the left and right halves helps further isolate or make independent
 these two halves.
 In the example, it is presumed that 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 might capture at least some of the idea that the left side of the sentence
 is about humans, and the right side is about human activities.
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)\ne p\left(w_{i},w_{j}\right)$
\end_inset

 and that the probability depends on the relation 
\begin_inset Formula $r$
\end_inset

.
 That is, this factor is presumed to not be a simple word-pair co-occurance
 probability 
\begin_inset Formula $p\left(\mbox{"goes"},\mbox{"fishing"}\right)$
\end_inset

, but also includes a weighting for this word-pair being an auxiliary-verb
 pair.
 This now gives a first hint of the appearance of semantics in a syntactic
 discussion: The 
\begin_inset Formula $p\left(w_{i},w_{j}\right)$
\end_inset

 captures a syntactic relation between a pair of words; the 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 captures something more.
\end_layout

\begin_layout Standard
The notation 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 may feel strange; a more conventional approach would be to write this as
 a conditional probability: 
\begin_inset Formula $\left(w_{i},w_{j}\vert r\right)$
\end_inset

 and read this as 
\begin_inset Quotes eld
\end_inset

the pair 
\begin_inset Formula $\left(w_{i},w_{j}\right)$
\end_inset

 conditioned on the relation 
\begin_inset Formula $r\left(w_{i},w_{j}\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 But this seems awkward, and invites inappropriate applications of Bayes
 theorem.
 For the present case, the non-standard notation used here seems easier
 to write and more direct to think about.
 It can always be re-imagined as conditional probabilities, on an as-needed
 basis.
 
\end_layout

\begin_layout Subsubsection*
Connectors and Disjuncts
\end_layout

\begin_layout Standard
Syntactic relations are not just pair-wise connections, though.
 Syntactic elements have more complex structure.
 Thus, the above factorization might be more correctly written as
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx\left[p\left(w_{1},\cdots,w_{i}\right)\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}\right]\times\left[\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}p\left(w_{i+1},\cdots,w_{n}\right)\right]
\]

\end_inset

so that half of 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 rides along with the left side, as the probability of making a connection,
 and the other half rides with the other side.
 This square root 
\begin_inset Formula $\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}$
\end_inset

 term can be refered as the 
\begin_inset Quotes eld
\end_inset

connector probability
\begin_inset Quotes erd
\end_inset

.
 The 
\begin_inset Quotes eld
\end_inset

connectors
\begin_inset Quotes erd
\end_inset

 are then locations in the factorized tensor that have the potential to
 make a connection.
 This apportions the probability away from the actual connection, from the
 actual linkage, and moves it to the two endpoints of the connection.
\end_layout

\begin_layout Standard
Of course, real grammatical relations are more complex; they are not just
 a compendium of pair-wise relationships.
 For example, a transitive verb 
\emph on
must
\emph default
 make a connection to both a subject on the left and an object on the right.
 This is effectively a triple 
\begin_inset Formula $\left(S,V,O\right)$
\end_inset

.
 Any factorizations involving transitive verbs should be factored in terms
 of a tri-variable 
\begin_inset Formula $p\left(\mbox{TrVb}\left\{ S,V,O\right\} \right)$
\end_inset

.
 A transitive verb 
\begin_inset Formula $V$
\end_inset

 has the possibility of connecting to a subject 
\begin_inset Formula $S$
\end_inset

, and the possibility of connecting to an object 
\begin_inset Formula $O$
\end_inset

.
 
\end_layout

\begin_layout Standard
This last paragraph is a sneaky introduction to Link Grammar.
 The specific grammatical relation is 
\begin_inset Formula $\mbox{TrVb}\left\{ S,V,O\right\} =\mathtt{V:S-\,\&\,O+}$
\end_inset

.
 The right-hand side is the conventional Link Grammar notation stating that
 the lexical entry 
\begin_inset Formula $\mathtt{V}$
\end_inset

 has the connector 
\begin_inset Formula $\mathtt{S-}$
\end_inset

 pointing to the left, and the connector 
\begin_inset Formula $\mathtt{O+}$
\end_inset

 pointing to the right.
 The combined expression 
\begin_inset Formula $\mathtt{S-\,\&\,O+}$
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

; the name stems from it's being disjoined from other lexical entries for
 the verb 
\begin_inset Formula $\mathtt{V}$
\end_inset

.
\end_layout

\begin_layout Standard
The other sneaky thing being done above is to introduce the idea of a 
\begin_inset Quotes eld
\end_inset

word class
\begin_inset Quotes erd
\end_inset

 (subjects, verbs, objects).
 Thus, during factorization, we expect to see indicator functions, such
 as 
\begin_inset Formula $p\left(w_{j}\in\mathtt{V}\right)$
\end_inset

 which takes a value of 1 if 
\begin_inset Formula $w_{j}$
\end_inset

 is verb, and zero, otherwise.
 Keep in mind, though, that it might be useful to assign fractional values
 to 
\begin_inset Formula $p\left(w_{j}\in\mathtt{V}\right)$
\end_inset

, for any number of technical reasons.
 It is premature to sketch these reasons, just yet.
\end_layout

\begin_layout Standard
The intendeed factorization, for transitive verbs, is to say that a tri-variable
 probability 
\begin_inset Formula $p\left(\mbox{TrVb}\left\{ w_{i,}w_{j},w_{k}\right\} \right)$
\end_inset

 can be factored into a a probability 
\begin_inset Formula $p\left(w_{i}\in\mathtt{S+}\right)$
\end_inset

 of 
\begin_inset Formula $w_{i}$
\end_inset

 belonging to some (any) class of words that can make subject-type connections,
 a probability 
\begin_inset Formula $p\left(w_{k}\in\mathtt{O-}\right)$
\end_inset

 of 
\begin_inset Formula $w_{k}$
\end_inset

 belonging to some (any) class of words that can make object-type connections,
 a probability 
\begin_inset Formula $p\left(w_{j}\in\mathtt{V}\right)$
\end_inset

 of 
\begin_inset Formula $w_{j}$
\end_inset

 belonging explicitly to the transitive verb class 
\begin_inset Formula $\mathtt{V}$
\end_inset

, and an overall probability of observing the relation 
\begin_inset Formula $p\left(\mathtt{V:S-\,\&\,O+}\right)$
\end_inset

.
 How should this be writen? The resulting factorization must be consistent
 with the notion of 
\begin_inset Quotes eld
\end_inset

connector probabilities
\begin_inset Quotes erd
\end_inset

.
 It must also be consitent with the lexical entry 
\begin_inset Formula $\mathtt{N:S+\,or\,O-}$
\end_inset

, which states that there is a word-class of common nouns that can act as
 a subject, when to the left of a verb, or as an object, when to the right
 of a verb.
 But this is not the only such lexical entry with 
\begin_inset Formula $\mathtt{S+}$
\end_inset

 or with 
\begin_inset Formula $\mathtt{O-}$
\end_inset

 connectors: certainly, pronouns can make these connections as well.
 This is the reason for writing 
\begin_inset Formula $p\left(w_{i}\in\mathtt{S+}\right)$
\end_inset

 instead of 
\begin_inset Formula $p\left(w_{i}\in\mathtt{N}\right)$
\end_inset

: what matters is not that 
\begin_inset Formula $w_{i}$
\end_inset

 is a noun, but that 
\begin_inset Formula $w_{i}$
\end_inset

 can serve as the subject of a sentence.
\end_layout

\begin_layout Standard
One concludes that transitive verbs contribute a factor 
\begin_inset Formula 
\begin{align*}
p\left(\mbox{TrVb}\left\{ w_{i,}w_{j},w_{k}\right\} \right)= & p\left(\mathtt{V:S-\,\&\,O+}\right)p\left(w_{j}\in\mathtt{V}\right)p\left(w_{i}\in\mathtt{S+}\right)p\left(w_{k}\in\mathtt{O-}\right)\times\\
 & \qquad\qquad\sqrt{p\left(\mathtt{S}\left\{ w_{i},w_{j}\right\} \right)p\left(\mathtt{O}\left\{ w_{j,}w_{k}\right\} \right)}
\end{align*}

\end_inset

where each of the first four 
\begin_inset Formula $p$
\end_inset

's can be imagined to be zero or one, exactly, and a square-root probability
 for each word participating in a specific linkage.
 The square-root appears because there is a corresponding square root at
 the other side of the link.
\end_layout

\begin_layout Standard
To complete the example, consider the case where the subject and object
 are common nouns.
 Then these are covered by the lexical entry 
\begin_inset Formula $\mathtt{N:S+\,or\,O-}$
\end_inset

 consisting of two disjoined disjuncts: one that says common nouns can act
 as a subject: 
\begin_inset Formula $\mathtt{N:S+}$
\end_inset

 and another where they act as objects: 
\begin_inset Formula $\mathtt{N:O-}$
\end_inset

.
 That is, for the subject,
\begin_inset Formula 
\[
p\left(\mbox{Subj}\left\{ w_{i,}w_{j}\right\} \right)=p\left(\mathtt{N:S+}\right)p\left(w_{i}\in\mathtt{N}\right)p\left(w_{j}\in\mathtt{S-}\right)\sqrt{p\left(\mathtt{S}\left\{ w_{i},w_{j}\right\} \right)}
\]

\end_inset

and similarly for the object.
 A three-word sentence 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 which has 
\emph on
exactly one parse
\emph default
 as SVO then has the probability
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(w_{1,}w_{2},w_{3}\right)=p\left(\mbox{Subj}\left\{ w_{1,}w_{2}\right\} \right)p\left(\mbox{TrVb}\left\{ w_{1,}w_{2},w_{3}\right\} \right)p\left(\mbox{Obj}\left\{ w_{2,}w_{3}\right\} \right)
\]

\end_inset

A longer sentence, say, one with adverbs, adjectives and determiners, having
 a transitive verb will then have a block factor 
\begin_inset Formula $p\left(w_{i,}w_{j},w_{k}\right)$
\end_inset

 of this same form.
 
\end_layout

\begin_layout Standard
The goal here is to describe factorization along the lines of conventional
 linguistic grammars.
 Although an explicit Link Grammar notation is used, the arguments above
 can be transposed to any grammatical theory.
 The building blocks are simply the vertexes and edges that are drawn by
 that grammatical theory.
 The factorization above is two-fold.
 First, a graph corresponding to the links drawn by a (single) parse in
 that grammatical theory, in terms of grammatical classes, and an adjustment
 for the actual words employed.
\end_layout

\begin_layout Subsubsection*
Phrase Structure
\end_layout

\begin_layout Standard
The descriptions above are primarily couched in a Dependency Grammar
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Dependency grammar"
target "https://en.wikipedia.org/wiki/Dependency_grammar"
literal "false"

\end_inset

.
\end_layout

\end_inset

 setting.
 A few words are in order about Chomksian-style production grammars, such
 as Phrase-Structure Grammars
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Phrase sturcture grammar"
target "https://en.wikipedia.org/wiki/Phrase_structure_grammar"
literal "false"

\end_inset

.
\end_layout

\end_inset

.
 Such grammars consist of production rules, the first of which is conventionally
 
\begin_inset Formula $S\to NP,S\backslash NP$
\end_inset

, stating that a sentence 
\begin_inset Formula $S$
\end_inset

 consists of a noun phrase 
\begin_inset Formula $NP$
\end_inset

 and the rest of the sentence 
\begin_inset Formula $S\backslash NP$
\end_inset

.
 This can be directly mapped to the assertion that 
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)=p\left(NP\left\{ w_{1},\cdots,w_{i}\right\} \right)p\left(S\backslash NP\left\{ w_{i+1},\cdots,w_{n}\right\} \right)
\]

\end_inset

for some yet-to-be-determined word index 
\begin_inset Formula $i$
\end_inset

.
 Such phrase structure grammars are necessarily trees, as production rules
 do not allow the creation of graphs with loops.
 The leaves of these trees are necessarily the words in the (fully-parsed)
 sentence.
 These trees, however, are not strict dependency trees: they also have non-leaf
 vertexes, labelled by the production rule that produced everything below.
 This does nothing to change the overall conception of factorization: in
 the example above, the factor 
\begin_inset Formula $p\left(\mathtt{V:S-\,\&\,O+}\right)$
\end_inset

 plays the same role as a production rule vertex.
\end_layout

\begin_layout Subsubsection*
Cliques and Spanning Trees
\end_layout

\begin_layout Standard
What if there are multiple parses? How should this be understood? In short,
 as a many-worlds summation.
\end_layout

\begin_layout Standard
Given 
\begin_inset Formula $n$
\end_inset

 words in a sentence, consider first the clique or complete graph
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Complete graph"
target "https://en.wikipedia.org/wiki/Complete_graph"
literal "false"

\end_inset

.
\end_layout

\end_inset

 of degree 
\begin_inset Formula $n$
\end_inset

: this is the graph where every word, a vertex, is joined to every other
 word by an edge.
 A specific parse of the sentence then corresponds to a spanning tree
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Spanning tree"
target "https://en.wikipedia.org/wiki/Spanning_tree"
literal "false"

\end_inset

.
\end_layout

\end_inset

 of this graph.
 This tree can be described in terms of an indicator function
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Indicator function"
target "https://en.wikipedia.org/wiki/Indicator_function"
literal "false"

\end_inset

.
\end_layout

\end_inset

 on the edges of the clique.
 That is, make a list 
\begin_inset Formula $\left\{ E\right\} $
\end_inset

 of all of the edges 
\begin_inset Formula $E$
\end_inset

 in the complete graph, and then provide a function 
\begin_inset Formula $\delta\left(E\right)$
\end_inset

 that is zero or one on each edge.
 A specific parse 
\begin_inset Formula $T$
\end_inset

 then corresponds to a specific indicator function 
\begin_inset Formula $\delta_{T}:\left\{ E\right\} \to\left\{ 0,1\right\} $
\end_inset

.
 As a block factorization, one has that 
\begin_inset Formula 
\[
p\left(T\left\{ w_{1},w_{2},\cdots,w_{n}\right\} \right)=\prod_{\left(w_{i},w_{j}\right)\in T}p\left(r\left\{ w_{i},w_{j}\right\} \right)
\]

\end_inset

where 
\begin_inset Formula $T$
\end_inset

 is the set of edges where the indicator function is one.
 The per-edge factors 
\begin_inset Formula $p\left(w_{i},w_{j}\right)$
\end_inset

 may be indicator functions themselves, or may be weighted, or may be the
 result of a more complex factorization, as described in the previous section.
 That is, some of the pair-wise terms in the product should have been written
 as triples or quads: 
\begin_inset Formula 
\begin{equation}
p\left(T\left\{ w_{1},w_{2},\cdots,w_{n}\right\} \right)=\prod_{r\in T}p\left(r\left\{ w_{i},w_{j},\cdots,w_{k}\right\} \right)\label{eq:factorization}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
If there is more than one possible parse of a sentence, then presumably
 one parse is prefered over another, and each can be weighted, with some
 probability 
\begin_inset Formula $p\left(T\right)$
\end_inset

 for each parse 
\begin_inset Formula $T$
\end_inset

.
 Each of these contributes to the overall analysis of the sentence:
\begin_inset Formula 
\begin{equation}
p\left(w_{1},w_{2},\cdots,w_{n}\right)=\sum_{T\in\left\{ T\right\} }p\left(T\left\{ w_{1},w_{2},\cdots,w_{n}\right\} \right)\label{eq:summation}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This summation implies that, in general, 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 probably cannot be factored into blocks, although each term in the sum
 is explicitly block-factored.
 It might happen that all parses have a common sub-block; in this case,
 the sub-block can be pulled out of the summation, leaving only the ambiguous
 part inside the summation.
 Classic ambiguous parses are 
\begin_inset Quotes eld
\end_inset

I saw the man with the telescope
\begin_inset Quotes erd
\end_inset

, and so on.
 
\end_layout

\begin_layout Subsubsection*
Deformation Retracts
\end_layout

\begin_layout Standard
In mathematics, Homotopy Theory
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Homotopy theory"
target "https://en.wikipedia.org/wiki/Homotopy_theory"
literal "false"

\end_inset

.
\end_layout

\end_inset

 concerns the structure of spaces with non-trivial topologies.
 In the present situation, the primary concern is how to work with graphs
 that have cycles (loops) in them, as opposed to those that do not.
 Trees are the prototypical example of a graph that has a trivial homotopy:
 the edges can always be shortened, until the two end-points have been collapsed
 into one.
 This is termed a 
\begin_inset Quotes eld
\end_inset

deformation retract
\begin_inset Quotes erd
\end_inset

.
 In the present case, it can be understood to mean that the factorization
 of a graph along an edge can be written as if the edge has shrunk to a
 point.
 Explicitly:
\begin_inset Formula 
\[
p\left(w_{1},\cdots,w_{i}\right)p\left(r\left\{ w_{i},w_{j}\right\} \right)p\left(w_{i+1},\cdots,w_{j},\cdots,w_{n}\right)=p\left(w_{1},\cdots,K\right)p\left(K\right)p\left(w_{i+1},\cdots,K,\cdots,w_{n}\right)
\]

\end_inset

where a new kind of 
\begin_inset Quotes eld
\end_inset

word
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $K$
\end_inset

 has been introduced.
 It's a compound word, a multi-word expression, a set phrase, a phraseme,
 an idiomatic expression, an institutional expression.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Phraseme"
target "https://en.wikipedia.org/wiki/Phraseme"
literal "false"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
name "Multiword expression"
target "https://en.wikipedia.org/wiki/Multiword_expression"
literal "false"

\end_inset

 and 
\begin_inset CommandInset href
LatexCommand href
name "Idiom"
target "https://en.wikipedia.org/wiki/Idiom"
literal "false"

\end_inset

.
\end_layout

\end_inset

 Collections of words can be 
\begin_inset Quotes eld
\end_inset

retracted
\begin_inset Quotes erd
\end_inset

, congealed down to single lexical units.
\end_layout

\begin_layout Subsubsection*
Twines
\end_layout

\begin_layout Standard
A more problematic situation arises for graphs that have cycles.
 Although conventional phrase-structure and and dependency parses are trees,
 loops can appear in depdenency parses.
 The very simplest case would be an HSV parse, where H is the left-wall
 or head of the parse, S is the subject, a noun, and V is a verb: this forms
 a triangle: there are three edges.
 H is used to indicate both the dominant noun (the subject) and also the
 dominant verb.
 
\end_layout

\begin_layout Standard
Perhaps this can be seen more clearly in dependent or relative clauses.
 An example from the Link Grammar documentation:
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See the Link Grammar Guide-to-Links 
\begin_inset CommandInset href
LatexCommand href
name "Section R"
target "https://www.abisource.com/projects/link-grammar/dict/section-R.html"
literal "false"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Section B"
target "https://www.abisource.com/projects/link-grammar/dict/section-B.html"
literal "false"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Section S"
target "https://www.abisource.com/projects/link-grammar/dict/section-S.html"
literal "false"

\end_inset

, and 
\begin_inset CommandInset href
LatexCommand href
name "Section C"
target "https://www.abisource.com/projects/link-grammar/dict/section-C.html"
literal "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

             +--B------+
\end_layout

\begin_layout Plain Layout

             +-R-+--S--+
\end_layout

\begin_layout Plain Layout

             |   |     |
\end_layout

\begin_layout Plain Layout

        The dog  I  chased was black 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the above the R link points at the head noun of the relative clause;
 the B link connects to the head verb of the relative clause, and the S
 link is the conventional subject-verb link.
 Loops may be larger than triangles:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

             +-----B------+
\end_layout

\begin_layout Plain Layout

             +-R--+C-+-S--+
\end_layout

\begin_layout Plain Layout

             |    |  |    |
\end_layout

\begin_layout Plain Layout

        The dog  who I chased was black 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The C link connects head nouns to subordinating conjunctions.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikitionary, 
\begin_inset CommandInset href
LatexCommand href
name "subordinating conjunction"
target "https://en.wiktionary.org/wiki/subordinating_conjunction"
literal "false"

\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Such loops present a potential complication to factoring.
 In order to factor a block of text into a left and a right component, there
 are now two links to be cut.
 There are several ways in which to imagine this issue.
 One is to presume that the relative clause is an irreducible block of the
 form 
\begin_inset Formula 
\[
p\left(\mbox{RelCl}\left\{ w_{i},w_{j};w_{k},w_{m}\right\} \right)
\]

\end_inset

with 
\begin_inset Formula $i,j$
\end_inset

 linking to the left-hand block and 
\begin_inset Formula $k,m$
\end_inset

 linking to the right.
 In te last example, it would be 
\begin_inset Formula $w_{i}=\mathtt{dog}$
\end_inset

, 
\begin_inset Formula $w_{j}=\mathtt{who}$
\end_inset

, 
\begin_inset Formula $w_{k}=\mathtt{I}$
\end_inset

 and 
\begin_inset Formula $w_{m}=\mathtt{chased}$
\end_inset

, so that the entire loop of the relative clause is unreduced, and has four
 connectors grand-total, emanating from it: 
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename graphics/loop-4-legs.eps
	width 40text%

\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
Alternately, this loop is clearly composed four three-point vertexes and
 so following the earlier conventions, there is a pair probability for each
 linkage (there are four links: R,B,C and S), a square-root for each exposed
 connector (there are two: D for determiner, and S), and a vertex factor
 for each vertex:
\begin_inset Formula 
\begin{align*}
p\left(\mbox{RelCl}\left\{ w_{i},w_{j};w_{k},w_{m}\right\} \right)= & p\left(\mathtt{R}\left\{ w_{i},w_{j}\right\} \right)p\left(\mathtt{C}\left\{ w_{j},w_{k}\right\} \right)\times\\
 & \qquad p\left(\mathtt{S}\left\{ w_{k},w_{m}\right\} \right)p\left(\mathtt{B}\left\{ w_{i},w_{m}\right\} \right)\times\\
 & \qquad p\left(\mbox{Vertex}\right)\times\\
 & \qquad\sqrt{p\left(\mathtt{D}\left\{ \mathtt{the},w_{i}\right\} \right)p\left(\mathtt{S}\left\{ w_{i,}\mathtt{was}\right\} \right)}
\end{align*}

\end_inset

the factors ofwhich follows from the fuller diagram:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

          +----------S-------+
\end_layout

\begin_layout Plain Layout

          +------B------+    |
\end_layout

\begin_layout Plain Layout

      +-D-+-R-+-C-+--S--+    |
\end_layout

\begin_layout Plain Layout

      |   |   |   |     |    |
\end_layout

\begin_layout Plain Layout

     the dog who  I  chased was black   
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In order to not clog the above, the vertex factor was separated out:
\begin_inset Formula 
\begin{align*}
p\left(\mbox{Vertex}\right)= & p\left(\mathtt{who:R-\,\&\,C+}\right)\times\\
 & \quad p\left(\mathtt{I:C-\,\&\,S+}\right)\times\\
 & \quad p\left(\mathtt{chased:S-\,\&\,B-}\right)\times\\
 & \quad p\left(\mathtt{dog:D-\,\&\,R+\,\&\,B+\,\&\,S+}\right)
\end{align*}

\end_inset

As before, the individual word-mentions could have been pulled out into
 grammatical classes, so that, for example:
\begin_inset Formula 
\[
p\left(\mathtt{chased:S-\,\&\,B-}\right)=p\left(\mathtt{chased}\in\mathtt{V}\right)p\left(\mathtt{V:S-\,\&\,B-}\right)
\]

\end_inset

and so on.
\end_layout

\begin_layout Subsubsection*
Summary
\end_layout

\begin_layout Standard
In this way, and ordinary seven-word sentence 
\begin_inset Quotes eld
\end_inset

the dog I chased was black
\begin_inset Quotes erd
\end_inset

, having a joint probability 
\begin_inset Formula $p\left(w_{1},\cdots,w_{7}\right)$
\end_inset

 can be factored into independent blocks.
 This factorization is more complex than a conventional Hidden Markov Model,
 and properly should be called a Markov random field.
 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Hidden Markov model"
target "https://en.wikipedia.org/wiki/Hidden_Markov_model"
literal "false"

\end_inset

, 
\begin_inset CommandInset href
LatexCommand href
name "Markov random field"
target "https://en.wikipedia.org/wiki/Markov_random_field"
literal "false"

\end_inset

and 
\begin_inset CommandInset href
LatexCommand href
name "Bayesian network"
target "https://en.wikipedia.org/wiki/Bayesian_network"
literal "false"

\end_inset

.
\end_layout

\end_inset

 We stop short of calling this a Bayesian Network, because we've stopped
 short of using any Bayesian priors 
\begin_inset Formula $p\left(\theta\right)$
\end_inset

, nor of combining them with liklihoods 
\begin_inset Formula $p\left(x\vert\theta\right)$
\end_inset

 to obtain posteriors 
\begin_inset Formula $p\left(\theta\vert x\right)\sim p\left(x\vert\theta\right)p\left(\theta\right)$
\end_inset

.
\end_layout

\begin_layout Standard
One reason to avoid Bayesian priors is to instead allow the use of Gibbs
 measure
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Gibbs measure"
target "https://en.wikipedia.org/wiki/Gibbs_measure"
literal "false"

\end_inset

.
\end_layout

\end_inset

 and so to rephrase the probabilities in terms of entropies (or free energies):
\begin_inset Formula 
\[
p\left(x_{i}\right)=\frac{1}{Z}e^{-\beta_{i}E_{i}}
\]

\end_inset

This also distinguishes the graphs here from the concept of Conditional
 Random Fields
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Conditional random field"
target "https://en.wikipedia.org/wiki/Conditional_random_field"
literal "false"

\end_inset

.
\end_layout

\end_inset

 which are conventionally formulated in terms of priors.
 Another reason to avoid Bayesian formulations is the problem of ambiguity,
 sketched below.
 
\end_layout

\begin_layout Subsubsection*
Many Worlds
\end_layout

\begin_layout Standard
The factorizations being described above are 
\emph on
not those of maximum entropy approaches
\emph default
! 
\emph on
Nor are they Bayesian
\emph default
! We must now be careful not to discard the baby with the bathwater: most
 of the equations above are a stream of bathwater; the baby is eqn.
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:summation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 That is, we are 
\emph on
not pursing a singular and best parse
\emph default
 (arrived by via MaxEnt or via Bayesian inference.) The goal is to explicitly
 avoid a single model.
 Avoid some Bayesian distribution over likelihoods.
 The central and key idea that we are struggling to expose is that one must
 insist that all of the various likelihoods are not only possible, but are
 intertwined.
 There is no one true reality that is merely unknown and needs to be found
 out; there are many, and they are necessarily tangled together.
\end_layout

\begin_layout Standard
Perhaps this sounds like philosophical quantum woo.
 It is not meant to be; it needs to be unpacked.
 Linguistics is relatively arid, but there are a few examples: again , the
 classic 
\begin_inset Quotes eld
\end_inset

I saw the man with the telescope.
\begin_inset Quotes erd
\end_inset

 In a spy thriller, perhaps the protagonist is looking through the telescope.
 In the biography of a famous astronomer, the proganist may be standing
 in an observatory.
 Without that context, it is ambiguous.
 
\begin_inset Quotes eld
\end_inset

Ah ha!
\begin_inset Quotes erd
\end_inset

 you may say, 
\begin_inset Quotes eld
\end_inset

but eventually it becomes clear, one or the other! And we can update our
 priors when it becomes clear!
\begin_inset Quotes erd
\end_inset

.
 Alas, it will never become clear.
 I will not tell you which context I am actually thinking of; you will be
 left hanging.
 I won't tell you, not because I'm secretive or coy, but because this is
 a meta-conversation about linguistics and not about telescopes.
 In this situation, it is fundamentally impossible for you to update your
 priors.
\end_layout

\begin_layout Standard
Poetry provides a richer example: what was the poet thinking, when he, she,
 wrote those verses? What did they want you, the reader, to think? A good
 poet will want you to think of a multitude of things, to feel many emotions,
 all at once, at the same time.
 The name of the game is not to update our Bayesian priors and select one
 and only one emotion on which we will fiercely focus (unless, of course,
 it is nationalistic poetry üòÉüòÉ).
\end_layout

\begin_layout Standard
More broadly, ambiguity is primal to common sense: our visual field is filled
 with a myriad of items, a rainbow of events and happenings.
 There's (usually) no particular reason for focus attention on one or another;
 the brain, in default mode, wanders across the field of possibilities.
 A mathematical formulation of AGI must also capture this freedom to wander
 about.
 And yet, also, some things are distinct.
 Distinctness is captured in eqn 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factorization"
plural "false"
caps "false"
noprefix "false"

\end_inset

: my coffee cup is distinct from my coffee.
 The world of possibilities is captured in eqn 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:summation"
plural "false"
caps "false"
noprefix "false"

\end_inset

: there are many and they compete with one-another.
\end_layout

\begin_layout Standard
If one is given only the string 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 and the task of factoring it, then sure, use eqn 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:factorization"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and be prepared to lose, whenever there are any admixtures of anything
 else in there.
 But there always will be admixtures of something else! Those admixtures
 cloud the factors, because the admixtures are entangled into the total.
 In the end, all language is necessarily poetry, an artful attempt to capture
 vague thoughts and set them into words, in such a way that someone else
 might happen upon them, and perhaps extract something meaningful, perhaps
 what the author meant, and yet unavoidably unclear, because the author
 was never precise enough, and the reader was never clever enough to understand.
 This is the human condition.
\end_layout

\begin_layout Standard
To reiterate: the goal here is both to factor, to obtain factors that are
 relatively unambiguous, and then at the same time, bracket the ambiguities
 so that they are each corraled in their own paddock, and can be recombined
 as needed.
 This need is what makes the machinery daunting.
\end_layout

\begin_layout Standard
Although neuroscientists will eventually find the neurological basis for
 human ambiguity, the origin of amiguity is not the human mind.
 Ambiguity is fundamental, in nature, as it stands.
 This is the message delivered by eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:binomial-MI"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and eqn.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:summation"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection*
MST Factorization
\end_layout

\begin_layout Standard
The sections above provide theoretical arguments.
 Let quiickly review the experimental situation.
 The Yuret-style MST factorization being used in the language-learning effort
 is effectively the presumption that
\begin_inset Formula 
\[
MI\left(w_{1},w_{2},\cdots,w_{n}\right)\approx\sum_{\left(w_{i},w_{j}\right)\in MST}MI\left(w_{i},w_{j}\right)
\]

\end_inset

This sum runs only over the maximum spanning tree, and contains only 
\begin_inset Formula $n-1$
\end_inset

 links.
 The corresponding complete graph would have 
\begin_inset Formula $n\left(n-1\right)/2$
\end_inset

 links in it, and so most of these are ignored.
 Specifically, the presumption is that these ignored links actually cancel
 higher-order terms in the full MI expansion.
\end_layout

\begin_layout Standard
Put more plainly, it appears that
\begin_inset Formula 
\[
\log_{2}\frac{p\left(w_{1},w_{2},\cdots,w_{n}\right)}{p\left(w_{1}\right)p\left(w_{2}\right)\cdots p\left(w_{n}\right)}
\]

\end_inset

is, in general, 
\begin_inset Quotes eld
\end_inset

freakishly high
\begin_inset Quotes erd
\end_inset

, and that much of it can be 
\begin_inset Quotes eld
\end_inset

knocked down to size
\begin_inset Quotes erd
\end_inset

 by using the MST, instead.
 
\end_layout

\begin_layout Standard
Lacking is a coherent theoretical argument as to 
\emph on
why
\emph default
 an MST parse provides a reasonable approximation to the factorization.
 Also lacking is any comprehensive experimental exploration comparing the
 full, formal factorization to the MST approximation.
 Gut feel implies that MST is OK or even 
\begin_inset Quotes eld
\end_inset

pretty good
\begin_inset Quotes erd
\end_inset

, but no one has characterized the structure of the difference
\begin_inset Formula 
\[
\Delta\left(w_{1},w_{2},\cdots,w_{n}\right)=MI\left(w_{1},w_{2},\cdots,w_{n}\right)-\sum_{\left(w_{i},w_{j}\right)\in MST}MI\left(w_{i},w_{j}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
When is 
\begin_inset Formula $\Delta$
\end_inset

 small? When is it large? What does it mean, when it is large? Are there
 tricks that can describe such deviations? 
\end_layout

\begin_layout Standard
The next step is, of course, to use disjuncts, but again, without any clear
 argument about why the disjuncts provide a good approximation to the higher
 order terms in the sum of binomial-MI equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:binomial-MI"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
More precisely, the binomial-MI equation is correct, but unweildy, because
 it contains large cancelling terms.
 What is unclear is why these terms cancel, and how to best obtain a diagonalize
d, factorized perturbative expansion.
 The MST->disjunct path is just a gut-feel approach to obtaining that perturbati
ve expansion.
 It lacks formal justification for why it works.
\end_layout

\begin_layout Section*
Factorization as Dimensional Embedding
\end_layout

\begin_layout Standard
Syntax alone is not enough to convey the meaning of an expression, and so
 the above approximations, of working with parses, are necessarily mediocre.
 The factorization we are groping for should more properly be written as
 a change of variable
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(r_{1},r_{2},\cdots,r_{k}\right)
\]

\end_inset

where the variables 
\begin_inset Formula $r_{1},r_{2},\cdots,r_{k}$
\end_inset

 are in some sense 
\begin_inset Quotes eld
\end_inset

more independent
\begin_inset Quotes erd
\end_inset

 than the word-sequence 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Note that in general, 
\begin_inset Formula $k\ne n$
\end_inset

.
 For spanning tree parses, the 
\begin_inset Formula $r_{i}$
\end_inset

 are understood to be links between word-pairs, and so 
\begin_inset Formula $k$
\end_inset

 is counting the number of links in the parse.
 Thus 
\begin_inset Formula $k=n-1$
\end_inset

 for word-pair relationships.
 If the parse has fundamental cycles (loops), then 
\begin_inset Formula $k=n-1+\ell$
\end_inset

 where 
\begin_inset Formula $\ell$
\end_inset

 is the number of fundamental cycles (
\emph on
e.g.

\emph default
 in an MPG parse).
\end_layout

\begin_layout Standard
Note that parsing performs a 
\begin_inset Quotes eld
\end_inset

dimensional oxidation
\begin_inset Quotes erd
\end_inset

: there are far more 
\begin_inset Formula $r_{i}$
\end_inset

's than there are 
\begin_inset Formula $w_{i}$
\end_inset

's.
 If the size of the base vocabulary is 
\begin_inset Formula $\mathcal{O}\left(\left|w\right|\right)\sim N$
\end_inset

 then 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{2}$
\end_inset

 where I'm using 
\begin_inset Formula $\mathcal{O}$
\end_inset

 notation because counting the size of the vocabulary is hard, when vocabulary
 words have a Zipfian distribution.
 Also, the claim that 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{2}$
\end_inset

 is somewhat misleading.
 Its actually more like 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma<2$
\end_inset

 because this is what the Zipfian distributions do to us.
 We've seen this experimentally, when we measure the sparsity and rarity,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See e.g.
 page 34 or Diary Part Five for rarity.
 I could have sworn I had this in othr tables, but I can't find it right
 now.
\end_layout

\end_inset

 but we haven't explicitly measured this.
\end_layout

\begin_layout Standard
Thus, MST parsing is a form of dimensional embedding, where the strings
 living in the relatively low-dimensional space 
\begin_inset Formula $\mathcal{O}\left(\left|\left(w_{1},w_{2},\cdots,w_{n}\right)\right|\right)\sim N^{n}$
\end_inset

 are embeded into the vastly larger space 
\begin_inset Formula $\mathcal{O}\left(\left|r_{1},r_{2},\cdots,r_{k}\right|\right)\sim N^{2k}$
\end_inset

.
\end_layout

\begin_layout Standard
In Link Grammar parsing, the embedding is not into a word-pair space, but
 into a disjunct space, which is explosively larger.
 That is, the relations 
\begin_inset Formula $r$
\end_inset

 are actually disjuncts 
\begin_inset Formula $d$
\end_inset

.
 I assume its 
\begin_inset Formula $\mathcal{O}\left(\left|d\right|\right)\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma>2$
\end_inset

, but again, we've monitored this size without actually ever measuring it's
 scaling dependence.
 This is an experimental to-do: got to fix that.
\end_layout

\begin_layout Subsection*
Paths in hyperspace
\end_layout

\begin_layout Standard
Lets try to paint a mental image of this.
 Consider a vector space of dimension 
\begin_inset Formula $N$
\end_inset

, with 
\begin_inset Formula $N$
\end_inset

 the size of the vocabulary.
 Each 
\begin_inset Formula $w_{k}$
\end_inset

 is then a unit vector 
\begin_inset Formula $e_{k}$
\end_inset

 in this space.
 The word-sequence 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 is a path in this space.
 The probabilities 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 are hard to factorize, because there are many of these paths, and they
 overlap a lot.
\end_layout

\begin_layout Standard
Consider now a vector space of dimension 
\begin_inset Formula $D$
\end_inset

, with 
\begin_inset Formula $D$
\end_inset

 being the number of disjuncts.
 Very roughly, 
\begin_inset Formula $D\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma>2$
\end_inset

 or somethig like that.
 So this is a much larger space.
 A single Link Grammar parse of a sentence 
\begin_inset Formula $S=\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 provides a unique sequence of disjuncts 
\begin_inset Formula $G=\left(d_{1},d_{2},\cdots,d_{n}\right)$
\end_inset

 fixed by that parse.
 As before, 
\begin_inset Formula $d_{1},d_{2},\cdots,d_{n}$
\end_inset

 specifies a path through the disjunct space.
 However, this time, the space is much larger, and so the accidental intersectio
n of two different paths is much less likely.
 There's disambiguation.
\end_layout

\begin_layout Standard
Another difference is that the path 
\begin_inset Formula $d_{1},d_{2},\cdots,d_{n}$
\end_inset

 is constrained.
 A syntactically valid path 
\begin_inset Formula $d_{1},d_{2},\cdots,d_{n}$
\end_inset

 is necessarily one where 
\emph on
all
\emph default
 of the connectors on all of the disjuncts 
\begin_inset Formula $d_{i}$
\end_inset

 in that path are fully connected.
 Other paths are simply not valid.
 This stands in sharp contrast to word sequences 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 which are unconstrainted: one is free to write any word-sequence, even
 if it's nonsense.
\end_layout

\begin_layout Subsection*
Metric spaces
\end_layout

\begin_layout Standard
The space of words is endowed with several metrics.
 One of the simplest ones is given by the word-pair MI.
 Fixing a word 
\begin_inset Formula $w$
\end_inset

, consider the vector 
\begin_inset Formula $\vec{w}$
\end_inset

 of length 
\begin_inset Formula $2N$
\end_inset

, whose vector components are given by 
\begin_inset Formula $x_{j}=MI\left(w,w_{j}\right)$
\end_inset

 for 
\begin_inset Formula $j<N$
\end_inset

 and by 
\begin_inset Formula $x_{j}=MI\left(w_{j},w\right)$
\end_inset

 for 
\begin_inset Formula $N\le j<2N$
\end_inset

.
 That is, using entirely conventional notation, write 
\begin_inset Formula $\vec{w}=\sum_{j}x_{j}\hat{e}_{j}$
\end_inset

 with the 
\begin_inset Formula $x_{j}$
\end_inset

 being just real numbers, and the 
\begin_inset Formula $\hat{e}_{j}$
\end_inset

 being the unit basis vector for the vector space.
 
\end_layout

\begin_layout Standard
Experimentally, it has been seem that the distribution of the MI of word-pairs
 is approximately Gaussian, perhaps even to a surprising degree.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Diary part Five, Part Nine, the 2008 word-pairs report and also the
 AGI 2022 paper.
\end_layout

\end_inset

 This implies that the word vectors 
\begin_inset Formula $\vec{w}$
\end_inset

 are uniformly randomly distributed on a unit sphere: that is, the word-vectors
 form a Gaussian Orthogonal Ensemble (GOE).
 Because these vectors are distributed on a sphere, the cosine distance
 between the vectors can be used as a metric.
 Intuitively, this metric judges two words to be similar, when they have
 similar neighbors.
\end_layout

\begin_layout Standard
This is not the only such metric.
 One can construct a different word-pair MI, from disjuncts.
 This is obtained by considering a pair-wise correlation 
\begin_inset Formula $f\left(w_{i},w_{j}\right)=\sum_{d}p\left(w_{i},d\right)p\left(w_{j},d\right)$
\end_inset

 and then constructing an MI from 
\begin_inset Formula $f$
\end_inset

.
 The sum over 
\begin_inset Formula $d$
\end_inset

 is the sum over disjuncts.
 This, too, appears to be dscribed by a GOE.
 Intuitively, this metric judges two words to be similar, when they appear
 in similar grammatical contexts.
 This is a bit stronger and more constrained than saying the words have
 similar neighbors: they must also be grammatically similar.
\end_layout

\begin_layout Subsection*
Nearby paths
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Nine of the diary.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "lang"
options "tufte"

\end_inset


\end_layout

\end_body
\end_document
