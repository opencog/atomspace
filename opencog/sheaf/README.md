
Network Inference and Analysis Tools
====================================

In this project, there's a generic theme of inferring structure from
a sequence of events.  That is, a sequence of events is observed in the
external world, and the question arises: are these events correlated?
Do they mean something when observed together, or is it happenstance?
What is the relationship between the items in the sequence of events?

The presumption made here is that the sequence of observed events are
being generated by multiple different actors, who are performing actions
independently of one-another, and sometimes exchanging messages with
one-another, along a network.  In the simplest case, the observed events
are the sequence of actions performed by each of the actors, placed in
some time-like sequential order.  The goal of the code in this directory
is to statistically infer the structure of the network, given a sequence
of observed events.

Theoretical computer science has explored a number of theories for
describing the relationships between ordered events, and their
interpretation as a network; these theories are inter-related, and
go under the name of:

 * Sequent Calculus (proof theory)
 * Process Calculus
 * Calculus of Communicating Systems (CCS)
 * The theory of Communicating Sequential Processes (CSP)
 * History monoids, trace monoids and Trace theory
 * Actor model
 * Dependency grammar
 * Sheaf theory

See the respective Wikipedia articles on each of these topics. A
lightning review of these concepts is given below.


Networks and Sheaf theory
-------------------------
The topological structure of a graph can be understood locally in terms
of "sheaf theory". In this framework, instead of looking at a graph as
whole, one instead looks at it locally, in terms of how any given vertex
attaches to the other vertexes around it.

Thus, isolating a single vertex in the graph, one can see that it has N
different edges attaching it to other vertexes in the graph: one says
that the degree of the vertex is N.  The "section" of the vertex is the
list of the N edges that are attached to it.  The graph, as a whole,
is then described by listing each vertex in it, and the section
associated with it.

This is not the only way to describe a graph: more simply, more
traditionally, it is enough to just list all of the vertexes and all
of the edges. However, by looking at a graph in terms of sections,
one can hope to apply all of the tools of sheaf theory to the problem.

Thus, a typical section might look like this:
```
    Section
        LexicalAtom "something"
        ConnectorSeq
            Connector
                LexicalAtom "it's"
                ConnectorDir "-"
            Connector
                LexicalAtom "curious"
                ConnectorDir "+"
```

The above encodes the idea that the vertex "something" has an edge that
connects it to the vertex "it's", and another edge that connects it to
the vertex "curious".

The `Section`, `ConnectorSeq`, `Connector` and `ConnectorDir` are real
atom types.  The `LexicalAtom` is not: its just an example. The word
"lexical" is used here to suggest that the above has the form of a
dictionary entry: one can look up "something" in the dictionary, and,
obtain as it's definition, the `ConnectorSeq` of everything it attaches
to.

The `ConnectorDir` will be explained later. In general, one may want to
include additional information about a connector: a weight, a distance,
its commutativity properties, etc.


Stalks, Germs, Lexical Items
----------------------------
A base presumption here is that a given vertex participates not in just
one network graph, but in millions of them. A single network can then be
viewed as a single global section of sheaf; it is the abstract collection
of all of the networks that comprises the sheaf.

By making a large number of statistical observations of graphs, and
then collecting statistics on sections, one can hope to discern how that
vertex typically connects into a typical graph. In sheaf theory, this
information about the typical behavior of a vertex is called the "stalk"
or the "germ" of the vertex. (A "stalk", because its like a plant, with
different branches forking out from the main trunk.  A "germ", because
it can grow and "germinate" different connections.) In linguistics, it
is called a "lexical entry" or "lexical item", because it resembles an
entry in a dictionary (a "lexis"): one can look up the vertex in the
dictionary, and get, as it's definition, a list of the kinds of edges it
participates in.

In Link Grammar, the disjoint union of all sections are called "connector
sets", and connector sequences are called "disjuncts". (Viz, each
connector sequence is disjoined from the other: to parse a graph, one
must choose one section, and discard all the others.)

The "gluing axiom" of a sheaf can be thought as describing how different
connectors can be joined together.  The act of parsing requires selecting
a single disjunct out of the disjoint union of all of them; thus, the
disjoint union is a set of 'possibilities' or parts of a 'possible
world', and so can be understood in terms of Kripke-Joyal semantics.


Grammar
-------
In linguistics and in computer science, the stalk/germ can be viewed as
a grammatical entry in a grammar.  A grammar describes how a "word" in
a "language" can be related to other words in the language.

In the theory of language, one is presented with a linear sequence of
words, a "sentence".  There are two primary styles for expressing
grammars: production grammars and dependency grammars. Production
grammars ("constituency grammars", "head-phrase structure grammars"
(HPSG) and so on) are expressed in terms of re-write or production rules.
These kinds of grammars are often classified according to the Chomsky
hierarchy: regular grammars that describe regular expressions (finite
state machines), context-free grammars that describe push-down automata,
etc.

A dependency grammar describes links or dependencies between the words
in a sentence. The prototypical dependency grammar is that of Tesnière.
Link Grammar is an example of a dependency grammar.  Note that, for
every production grammar, there is an equivalent dependency grammar,
and vice versa.  The lexis of a dependency grammar can be written as a
collection of sections; this is made very explicit in Link Grammar.

A language is then the same thing as the "étalé space" of a sheaf.


Trace theory
------------
In a language, sentences are linear sequences of words. The constraint
of linear. ordered sequences is loosened in trace theory, CSP and CCS.
Those theories describe "partially-commutative monoids", where the
concept of a sentence is replaced by the concept of a "trace". A trace
is a sequence, where the order of some of the items in the sequence is
not important.

An example from natural language might be:
```
   "This (maybe, is) an example."
```
which encodes the idea that the two sentences: "This maybe is an
example." is "This is maybe an example." have more or less the same
meaning, and that the word-order for `(maybe, is)` essentially did not
matter.

Relative order dependence/independence typically occurs in any situation
where there are instructions for performing actions.  Thus, "place
flour, salt and water in a bowl" does not specify what order these
ingredients are placed in the bowl; what does matter is that this is
performed before placing the mixture in the oven.  In computer science,
this describes the notion of parallel processing: some computations can
be done in parallel, as long as they are all completed before some other
computation is performed.  Serialization ("rendezvous") primitives are
mutexes and semaphores.

In trace theory, one has the general idea that one is observing traces
as the result of computations being performed by distinct agents or
actors exchanging messages between one-another; the ordering of some
messages does not matter; the order of others do.  The sequence of
observed messages is the "trace".


Trace and History Monoids
-------------------------
The example
```
   "This (maybe, is) an example."
```
is an example of a "trace monoid" (see Wikipedia).  For every trace
monoid, there is an equivalent (isomorphic) "history monoid". For this
example, it is
```
  [This, This] [maybe, .] [., is] [an, an] [example, example]
```
This makes clear that the network consists of two actors, both of
which move to the state "This", initially. Then one actor moves to
the state "maybe", while the other simultaneously moves to the state
"is". Both then move to the state "an", followed by "example".  The two
actors presumably exchange messages to accomplish this synchronization;
those messages are not visible in the trace; only the sequence of states
are.


Cause and effect
----------------
Whenever a sequence of events is observed for a system, the apparent
order of "cause" and "effect" can be reversed, with the apparent "cause"
coming long after the "effect".

For example: to build a high-rise building, a foundation must be dug
first.  Observed as events in time, the construction comes after the
foundation is built. However, it would be incorrect to say that a hole
in the ground "causes" a sky-scraper to appear, even though it came
earlier.  This is because the formal cause of the skyscraper is the
will of a real-estate developer; yet, this will is not observed; only
the construction events are.  From the point of view of a dependency
grammar describing the sequence of events, the skyscraper should be
viewed as the "head", and the hole in the ground as the "dependent",
with the head dominating the dependent, or "causing" the dependent,
even though the dependent comes earlier.

This example demonstrates why Hidden Markov Model (HMM) and Bayesian
network models of human language fail: The earlier words in a sentence
do not, cannot "cause" later words to appear; rather, it is often the
case that the later words "cause" or "force" the earlier words to appear.

More generally, this example shows why a dependency grammar approach,
with events associated with "sites", "germs", "stalks", "lexical
entries" is more appropriate for the analysis of a network, and is more
powerful, than HMM, Bayesian networks or Latent Semantic Analysis (LSA)
can be.

Random jab: this is also why Hutter's naive AIXI is incorrect: it only
considers past events, thus incorrectly inferring in a forward time-like
direction.  Clearly, this leads to incorrect conclusions about
skyscrapers, and fails to induce the Aristotelian "formal cause" of
events.


Inferring Grammar
----------------
To observationally infer the grammar of a network, one must observe a
lot of networks. With each network observation, one may create a set
of sections summarizing that network.  To get a view of the general
network, counts can be maintained for each observed section.  The result
of such counting is a frequency distribution over sections. To induce a
grammar, one may then compare the distributions on different vertexes;
if they are sufficiently similar, the vertexes can be grouped together
into a class. Since vertexes also occur as the end-points of connectors,
a grouping also has to be observed there.

If the network is not apparent (if it is latent or hidden), then one
cannot directly generate sections, because one cannot directly observe
the edges. In this case, a more round-about route is required, as
follows:

1) Assume all possible networks occur with equal probability.
2) Observe a lot of sequences, and count the frequency with which edges
   occur.
3) Compute the mutual information (MI) for each edge. That is, each edge
   has two endpoints, and the co-occurance of these endpoints can be
   captured as the mutual information between them.  The MI serves as
   a kind of measure of covariance or correlation.
4) Re-observe a lot of sequences, this time over-laying them with a
   maximal spanning tree (MST). So unlike step 1, where each network was
   assumed to be a clique, this time, the network is assumed to be a
   tree.  The "correct" tree is the tree that maximizes the sum of the
   MI of the edges.
5) Compute the sections of the MST, and accumulate these to obtain a
   distribution of sections.

The grammar can then be inferred from the distribution of the sections.


MST parsing
-----------
The primary tool in this directory is an MST parser. Given a specific
sequence of events, viz a sequence of atoms, all of the same type, and
given a large pool of observed dependencies between pairs of events, the
MST parser will construct a dependency tree such that the score of the
edges of the dependency tree are maximized.

Typical pair-wise relationships might be indicated as follows, in the
atomspace:
```
    EvaluationLink   (MI=24.3189)
        PredicateNode "word-pair"
        ListLink
            WordNode "foo"
            WordNode "bar"
```
which indicates that the word-pair (foo,bar) was observed with a mutual
information (MI) of 24.3189.

The atomspace can hold sparse matrix of such pair-wise data; in a
certain sense, the atomspace was designed from the get-go to do exactly
that.

The MST parse just creates a tree connecting all of the atoms in a
sequence, such that the sum-total (addition) of the scores of all the
edges in the tree is maximal, as compared to any other tree.

After the MST parse, the section for each vertex in the parse can be
computed.
